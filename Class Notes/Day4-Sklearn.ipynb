{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep, Scaling, Train-Test-Validate, and Pipelines.\n",
    "\n",
    "Today we will follow **Chapter 2 of our book: Hands-on Machine Learning**. We will work our way through the overview of a complete machine learning project. Our goals include:\n",
    "\n",
    "Understanding the importance of: \n",
    "* data cleaning, prep, and visualization\n",
    "* separating the data into training, testing, and validating sets\n",
    "* normalizing or scaling the data\n",
    "* how to set up a pipeline to do much of this work (advanced)\n",
    "\n",
    "We will also learn about the Python package sklearn:\n",
    "https://scikit-learn.org/stable/getting_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1u8T1tUeCCnNfMutsMKco6oUSs7mzuDvq?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Cell\n",
    "\n",
    "It is a good idea to just copy and past your preliminary cell into each new notebook once you have one that works for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Start a list of useful packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "# 1. Get the data - California Housing Prices \n",
    "</span>\n",
    "\n",
    "### GOAL - to predict median home price given other information.\n",
    "\n",
    "\n",
    "* In this class we will provide data sets for you to investigate. \n",
    "\n",
    "* There are lots of interesting machine learning data sets that you can explore here: https://archive.ics.uci.edu/ml/index.php\n",
    "\n",
    "* Sometimes the hardest part of an ML project is finding good, reliable, data and understanding the data that you have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you did not want to use Pandas you would do this:\n",
    "\n",
    "# import os\n",
    "# import tarfile\n",
    "# import urllib\n",
    "\n",
    "# DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "# HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "# HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "# def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "#     if not os.path.isdir(housing_path):\n",
    "#         os.makedirs(housing_path)\n",
    "#     tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "#     urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "#     housing_tgz = tarfile.open(tgz_path)\n",
    "#     housing_tgz.extractall(path=housing_path)\n",
    "#     housing_tgz.close()\n",
    "    \n",
    "# fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE DATA FOR TODAY\n",
    "\n",
    "# Exact location of the .csv file\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.csv\"\n",
    "\n",
    "# Load it using pandas\n",
    "housing = pd.read_csv(HOUSING_URL)\n",
    "# You could write a nice function to do this!\n",
    "\n",
    "# Start looking at what we have\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "# 2. Look at the data\n",
    "</span>\n",
    "\n",
    "Answer the following questions about the data:\n",
    "* What type of data is in each column?\n",
    "* How many entries are in our data set?\n",
    "* What are all of the column names?\n",
    "* What are the max, min, and mean of each column?\n",
    "* What are the counts for each item in the ocean_proximity column?\n",
    "* Are there any NaNs in the data? If so, look "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DON'T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEEK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "print('Data in each column:')\n",
    "print(housing.info())\n",
    "\n",
    "print('--------------------------------------')\n",
    "\n",
    "print('Entries in our data set:')\n",
    "print(housing.shape[0])\n",
    "\n",
    "print('--------------------------------------')\n",
    "\n",
    "print('Column names:')\n",
    "print(housing.columns)\n",
    "\n",
    "print('--------------------------------------')\n",
    "\n",
    "print(\"Data about max, min, mean, and more:\")\n",
    "display(housing.describe())\n",
    "\n",
    "print('--------------------------------------')\n",
    "\n",
    "print('Counts for itmes in ocean_proximity column:')\n",
    "print(housing[\"ocean_proximity\"].value_counts())\n",
    "\n",
    "print('--------------------------------------')\n",
    "\n",
    "print('Check for NaNs:')\n",
    "print(housing.isna().sum())\n",
    "\n",
    "print('--------------------------------------')\n",
    "\n",
    "print('Rows that contain NaNs')\n",
    "display(housing[housing['total_bedrooms'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "# 3.  Do some preliminary data visualization\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easiest way is to plot the data in matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check assumptions often!\n",
    "\n",
    "* Trustworthy source?\n",
    "* Any unusual information?\n",
    "* Could the data be biased or could there be errors?\n",
    "* Any unexpected values or units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "# 4. IMPORTANT: Test - Train Split\n",
    "\n",
    "</span>\n",
    "\n",
    "* At this point, once you trust and understand your data, you want to set aside some of your data for testing your model. \n",
    "* You will not \"look\" at this data again until your machine learning algorithm is read to test.\n",
    "\n",
    "### Training set: \n",
    "Usually about 80% of your data, to be used in building/training your model.\n",
    "\n",
    "### Testing set:\n",
    "Usually about 20% of your data, to be used in testing how well your model makes predictions on new data. This data could be split into testing and validation sets if you have a lot of parameter tuning to do - more about this later in the class.\n",
    "\n",
    "\n",
    "### SKLEARN\n",
    "One of the best basic machine learning python packages! \n",
    "\n",
    "Many of the algorithms that we will learn in class are part of the sklearn library. Remember **It is important to understading the underlying mathematics** if you want to do good machine learning. Please avoid falling into the black box machine learning mentality!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output identical at every run\n",
    "# you would not need this for individual projects - just for in class demos\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random samples:\n",
    "\n",
    "Spliting your data set randomly works well if you have a huge dataset or if you data set has very little bias. You are assuming that you will not introduce sampling bais by taking a random sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM\n",
    "# Use test_train_split to get two different datasets each randomly sampled:\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how we could get a training set with almost no median incomes at the higher end of the tail.\n",
    "housing[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Samples:\n",
    "\n",
    "You want to make sure your data samples match the overall population. Example: Your full dataset contains pictures of 700 cats and 300 dogs and you want to train a model that identifies cats from dogs. You could randomly sample a training set that contains 700 cats and 100 dogs and a test set that contains 200 dogs. Not good. Instead, stratified samples make sure that certain categories in the data are represented at the correct percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column in our Data Frame that categorizes the median income into levels with labels\n",
    "# 1 - ranges from 0-1.5\n",
    "# 2 - ranges from 1.5-3\n",
    "# 3 - ranges from 3-4.5\n",
    "# 4 - ranges from 4.5-6\n",
    "# 5 - anything higher than 6 (to np.inf)\n",
    "\n",
    "# Here we are chosing the categories - these are part of our model assumptions.\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data falls into each bin?\n",
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATIFIED\n",
    "# Here we split the data: \n",
    "\n",
    "# First we use sklearn to find the indices of how we would split our data for each column\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# n_splits=1 is 1-fold\n",
    "\n",
    "# Get the indicees that give us the stratified random split out of the split object\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see the different proportions\n",
    "\n",
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now delete the extra data column that we added - it's not really data we need later\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "# 5. Explore and visualize the TRAINING data to gain insights\n",
    "\n",
    "</span>\n",
    "\n",
    "* Here we are now only using the training set so that we keep the test set \"out of sight\" of our model\n",
    "* There are lots of different ways to visualize the data. \n",
    "* Our goal is to try to decide what kind of a machine learning model might work best and what variables are the best predictors\n",
    "\n",
    "Keep your question in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a clean copy of the training set\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple scatter plot\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A scater plot that causes high density points to be darker\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing.plot? # You can explore these functions!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A super fancy plot that:\n",
    "# -- sets the radius to represent population\n",
    "# -- sets a color map to housing proces\n",
    "# -- sets high density points to be darker\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.legend()\n",
    "\n",
    "# Here s=size and c=color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make some observations:\n",
    "\n",
    "* Looking at the data what trends do you see? \n",
    "* It's a good idea to write a paragraph or two about what you see in the data at this point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look to see which things are correlated with the variable of interest\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can corrrelation tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar graphs and scatter plots for the variables you are interested in.\n",
    "\n",
    "# Variables of interest\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "\n",
    "# use the Pandas scatter_matrix to make plots)\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which of these plots helps answer our question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose some of the highly correlated ones to zoom in on\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1)\n",
    "plt.axis([0, 16, 0, 550000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore some new attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new attributes\n",
    "# These are now columns in your training set\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n",
    "             alpha=0.2)\n",
    "plt.axis([0, 5, 0, 520000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"bedrooms_per_room\", y=\"median_house_value\",\n",
    "             alpha=0.2)\n",
    "plt.axis([0, 1, 0, 520000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "# 6. Prepare the data for Machine Learning algorithms - preprocessing\n",
    "\n",
    "</span>\n",
    "\n",
    "* Only now are we ready to start preparing for machine learning. \n",
    "* Notice how much data prep and exploration we have already done and there is still more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a clean version of the training data\n",
    "# Just in case you did something weird during exploration\n",
    "\n",
    "# FEATURES or INPUT PARAMETERS\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "# LABELS or OUTPUT PARAMETERS\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# We want to use the input parameters or features to predict median_house_values or labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider what to do with \"bad\" data or NaNs.\n",
    "\n",
    "Sometimes this is done at the top when you are looking at your data. That is okay too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find any rows in the training data that contain a NaN\n",
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What should we do with these NaNs?\n",
    "\n",
    "We have options:\n",
    "1. Just drop the rows that have a NaN - this must be done to both the Features and the Labels.\n",
    "2. Just get rid of the column that has NaNs and do let it be an input.\n",
    "3. Find some value to use to fill the NaNs - the median, max, min, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = housing[\"total_bedrooms\"].median() \n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True) # option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it worked\n",
    "housing[housing.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Be Careful\n",
    "\n",
    "1. Always calculate medians, max, etc based only on information from the training set. **Your model is not allowed to see the testing set!**\n",
    "2. If you remove rows from the features you must remove those rows from the labels.\n",
    "2. Remember what you did here! \n",
    "3. when it comes time to use the testing data, you will have to repete ALL of these steps exactly the same way.\n",
    "\n",
    "Eg. We would fillna in our test set with the SAME median found from our training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we deal with categorical (non numerical) data?\n",
    "\n",
    "Often our data sets contian data that is not numerical. In this dataset we have the input ocean_proximity. There are a few ways to deal with this:\n",
    "\n",
    "1. **Ordinal Encoding:** Create a single column in your dataset that numbers the categories. For ocean proximity we will have 0, 1, 2, 3, 4 to represent the five categories.\n",
    "\n",
    "2. **One Hot Encoding:** Create a column of arays that identify the category by zeros and ones. The first category will be [1,0,0,0,0] the next [0,1,0,0,0] and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the labels\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal Encoder - sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Send the categorical data through the encoder\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "# This produces an aray of values that we could add as a column in our Data Frame\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The categories are saved so that later you could reincode\n",
    "print(ordinal_encoder.categories_)\n",
    "\n",
    "# You can undo the encoding \n",
    "ordinal_encoder.inverse_transform(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding - sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Send the categorical data through the encoder\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "\n",
    "# Try to print the information\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `OneHotEncoder` class returns a sparse array, which saves memory by not saving zeros. This makes sense for one hot encoding, but is not great for users who want the data. This is why we set `sparse = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still save the categories\n",
    "cat_encoder.categories_\n",
    "\n",
    "# We can still invert\n",
    "cat_encoder.inverse_transform(housing_cat_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoder - sklearn - another way\n",
    "\n",
    "With everything you will see in this class there are **multiple ways to do the same thing**. Here is a fun package called LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define th encoder\n",
    "le=LabelEncoder()\n",
    "# Fit the classes\n",
    "le.fit(housing[\"ocean_proximity\"])\n",
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "le.transform(housing[\"ocean_proximity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL Add new features \n",
    "\n",
    "Remember if you want to use anything in your model, you would need to add it as a column to your features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as an Example:\n",
    "\n",
    "### If I watned to I could add some attributes and encodings to the dataset:\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "housing[\"one_hot_locations\"]=housing_cat_1hot.tolist()\n",
    "\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining NaNs\n",
    "display(housing[housing.isnull().any(axis=1)].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What have we done here:\n",
    "1. We got a fresh copy of the training data\n",
    "2. We broke it into Features (Inputs) and Labels (Outputs)\n",
    "3. We delt with NaNs in our data\n",
    "4. We added new features: new attributes and label encodings.\n",
    "\n",
    "\n",
    "#### If you are a more advanced programmer you cade defined your own class function that will fit and transform the data adding new attributes automatically! (see code at the bottom of this notebook)\n",
    "\n",
    "But this is not expected in this class. See the book for more infomation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "# 7. Feature Scaling\n",
    "\n",
    "</span>\n",
    "\n",
    "You pretty much **ALWAYS DO THIS**!! This is importnat with numerical data. With categorical not so much, you can usually leave those columns alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases you will want to make sure that each of your features (x) are normalized or scaled. What this means is that we want the data to have a few characteristics:\n",
    "\n",
    "- Each of the features has approximately the same scale\n",
    "- Most of your data falls between -1 and 1\n",
    "- This is not unique and there are lots of ways to do this... here are two\n",
    "\n",
    "- This helps numerical optimization methods converge faster and helps avoid over/under flow errors\n",
    "- Much more important in problems with many features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Mix Normalization\n",
    "\n",
    "The simplest method \n",
    "\n",
    "The general formula for a min-max of [0, 1] is given as:\n",
    "\n",
    "$$  x'={\\frac {x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}} $$\n",
    "\n",
    "This scales your data to 0 < x' < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Normalization\n",
    "\n",
    "Simple Mean normalization\n",
    "\n",
    "$$ x'={\\frac {x-{\\text{average}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}} $$\n",
    "\n",
    "\n",
    "### Standardization\n",
    "\n",
    "Similar to above just divide by the standard deviation. Ensures your data has zero mean and unit variance.\n",
    "\n",
    "$$ x'={\\frac {x-{\\text{average}}(x)}{{\\text{std}} (x)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scalar from sklearn\n",
    "scalar = StandardScaler()\n",
    "\n",
    "# This works only with numerical data: \n",
    "# housing_scaled = scalar.fit_transform(housing) # Gives an error\n",
    "# So we drop the categorical collumns to proceed\n",
    "housing_num = housing.drop(columns=[\"ocean_proximity\",\"one_hot_locations\"])\n",
    "housing_num.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_scaled = scalar.fit_transform(housing_num)\n",
    "housing_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "* How many FEATURES do we have?\n",
    "* How many data points do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could look at the data as a data frame just to confirm all the numbers look right\n",
    "checkdf = pd.DataFrame(housing_scaled)\n",
    "checkdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "# 8. Select and train a model \n",
    "\n",
    "</span>\n",
    "\n",
    "* The rest of our class will be concerned with developing different machine learning models. \n",
    "* You are expected to do all of the data exploration, splitting, and preparaion for each project you do!\n",
    "\n",
    "**Here is a simple example of doing machine learning on this data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Since we did all our data prep above, training is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose simple linear regression - you will learn abou this soon!\n",
    "\n",
    "# Define the machine learning model\n",
    "lin_reg = LinearRegression()\n",
    "# Learn....\n",
    "lin_reg.fit(housing_scaled, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "Remember all the data prep we did with the training data. We have to redo this on the testing data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get a copy the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model now \"knows\" the parameters that best fit the training data\n",
    "# Lets test the model on the testing data\n",
    "\n",
    "# We have to redo all the preprocessing steps we did to the training data\n",
    "\n",
    "# Get the test data\n",
    "mini_test_data = strat_test_set\n",
    "\n",
    "# Break into Features and Labels\n",
    "# FEATURES or INPUT PARAMETERS\n",
    "housing_test = mini_test_data.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "# LABELS or OUTPUT PARAMETERS\n",
    "housing_labels_test =  mini_test_data[\"median_house_value\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for NaNs and apply the same opperation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "housing_test[housing_test.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same NaN treatment - using the variable median from the training data\n",
    "housing_test[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs again\n",
    "housing_test[housing_test.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add any additional features that you used in training**\n",
    "\n",
    "Remember that even though we did some operations on the categorical data, in the end we dropped the categorical data from our features before we scaled the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new features or atributes that were used in training\n",
    "housing_test[\"rooms_per_household\"] = housing_test[\"total_rooms\"]/housing_test[\"households\"]\n",
    "housing_test[\"bedrooms_per_room\"] = housing_test[\"total_bedrooms\"]/housing_test[\"total_rooms\"]\n",
    "housing_test[\"population_per_household\"]=housing_test[\"population\"]/housing_test[\"households\"]\n",
    "\n",
    "housing_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numerical columns\n",
    "housing_num_test = housing_test.drop(columns=[\"ocean_proximity\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Standardization**\n",
    "\n",
    "You can reuse the scalar oblect that you defined above\n",
    "\n",
    "* Training use `fit_transform`\n",
    "* Testing use `transform` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input parameters\n",
    "housing_test_scaled = scalar.transform(housing_num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double check the shapes!**\n",
    "\n",
    "Your scaled features from the testing data should match the number of scaled features you had in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes\n",
    "print(\"TESTING DATA\")\n",
    "print(housing_test_scaled.shape)\n",
    "print(\"TRAINING DATA\")\n",
    "print(housing_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait! These don't match exactly! **WHY?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now send the data though the model to get predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now feed the test features into your model\n",
    "model_predictions = lin_reg.predict(housing_test_scaled)\n",
    "model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are these numbers? \n",
    "* What should our model predict? \n",
    "* How well did it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could just look at the difference\n",
    "percent_diff = np.abs((model_predictions-housing_labels_test))/housing_labels_test*100\n",
    "df_test = pd.DataFrame({\"model\":model_predictions,\"real data\":housing_labels_test,\"percent\": percent_diff})\n",
    "df_test.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will learn about many metrics for measuring the goodness of a model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_mse = mean_squared_error(housing_labels_test, model_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are clearly getting a lot of these wrong by quite a bit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "# 9. Fine-tune your model or choose a different model or maybe reconsider your data or...\n",
    "\n",
    "</span>\n",
    "\n",
    "Machine learning and mathematical modeling is a creative process. You have to use your instincts and make hard decisions. Sometimes your model just works great! Most of the time it does not and you go back and rethink your steps.\n",
    "\n",
    "**Some good advice**\n",
    "* keep track of your assumptions and decisions\n",
    "* comment your code\n",
    "* be really cynical about your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA - More advanced programming\n",
    "\n",
    "You will notice that we ended up doing a lot of things twice. Once to our training data and then again to our testing data. \n",
    "\n",
    "Often it is nice to define a special function that will do all of these things for you. \n",
    "\n",
    "Sklearn has something called a **pipeline** that will do this for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for training data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh copy of the data\n",
    "# FEATURES or INPUT PARAMETERS\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "# Drop categorical or string columns\n",
    "housing_num = housing.drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "# LABELS or OUTPUT PARAMETERS\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "\n",
    "# To automatically deal with NaNs use SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# To automatically add new attributes, define a class that does this\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# The class as writen below needs to know the column index\n",
    "# eg total rooms is in column 4 so to python this is index 3\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        # add new attributes\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pipeline allows you to do multiple operations one after another\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), #First we get rid of nans\n",
    "        ('attribs_adder', CombinedAttributesAdder()), #Then we add the new data\n",
    "        ('std_scaler', StandardScaler()), #Then we use standard scalar\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the data by sending it through the pipeline**\n",
    "\n",
    "`fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you defined all the thigns above\n",
    "# Then you can do all the stuff to the data in one line\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "housing_num_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do the same steps to the testing data by sending it through the pipeline**\n",
    "\n",
    "`transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh copy of the testing data\n",
    "# FEATURES or INPUT PARAMETERS\n",
    "housing_test = strat_test_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "# Drop categorical or string columns\n",
    "housing_num_test = housing_test.drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "# Do all the stuff again to the testing data\n",
    "housing_num_test_tr = num_pipeline.transform(housing_num_test)\n",
    "housing_num_test_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline that includes the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could even add the machine learning model to the pipeline\n",
    "# This still uses the classes we defined above\n",
    "\n",
    "# Get a fresh copy of the data\n",
    "# FEATURES or INPUT PARAMETERS\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "# Drop categorical or string columns\n",
    "housing_num = housing.drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "# LABELS or OUTPUT PARAMETERS\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), #First we get rid of nans\n",
    "        ('attribs_adder', CombinedAttributesAdder()), #Then we add the new data\n",
    "        ('std_scaler', StandardScaler()),#Then we use standard scalar\n",
    "        ('lin_reg', LinearRegression()),#Then do the model\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline.fit(housing_num,housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh copy of the testing data\n",
    "# FEATURES or INPUT PARAMETERS\n",
    "housing_test = strat_test_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "# Drop categorical or string columns\n",
    "housing_num_test = housing_test.drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "# Get the predictions in one line\n",
    "full_pipeline.predict(housing_num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced programming - scaling/normalizing with categorical data\n",
    "\n",
    "See the book chapter 2 for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are also methods for writing classes and function \n",
    "### for dealing with both categorical and numerical data\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "class OldDataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "    num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "old_num_pipeline = Pipeline([\n",
    "        ('selector', OldDataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "old_cat_pipeline = Pipeline([\n",
    "        ('selector', OldDataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', OneHotEncoder(sparse=False)),\n",
    "    ])\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "old_full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", old_num_pipeline),\n",
    "        (\"cat_pipeline\", old_cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
